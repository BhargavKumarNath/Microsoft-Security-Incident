{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ef44cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeda14e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Feature Engineering\n",
      "Step 1: Creating base incident table...\n",
      "Step 2: Engineering count and diversity features...\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA_PATH = '../data/raw/GUIDE_Train.parquet'\n",
    "PROCESSED_FEATURES_PATH = '../data/processed/incident_features.parquet'\n",
    "\n",
    "print(\"Starting Feature Engineering\")\n",
    "\n",
    "# Load data\n",
    "lazy_df = pl.scan_parquet(TRAIN_DATA_PATH)\n",
    "\n",
    "# 3. Create base dataframe for incidents\n",
    "print(\"Step 1: Creating base incident table...\")\n",
    "incident_base = (\n",
    "    lazy_df.select(['IncidentId', 'IncidentGrade']).drop_nulls().unique(subset=['IncidentId', 'IncidentGrade'])\n",
    ")\n",
    "\n",
    "# 4. Engineer Count and Diversity Features\n",
    "print(\"Step 2: Engineering count and diversity features...\")\n",
    "feature_expressions = [\n",
    "    pl.len().alias('evidence_count'),\n",
    "    pl.n_unique('AlertId').alias('unique_alert_count'),\n",
    "    pl.n_unique('EntityType').alias('unique_entity_type_count'),\n",
    "    pl.n_unique('DetectorId').alias('unique_detector_id_count'),\n",
    "    pl.n_unique('MitreTechniques').alias('unique_mitre_techniques_count'),\n",
    "    pl.n_unique('OrgId').alias('unique_org_id_count'),\n",
    "]\n",
    "\n",
    "incident_features = lazy_df.group_by('IncidentId').agg(feature_expressions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4330186e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Joining features to base table...\n",
      "Step 4: Saving processed features to ../data/processed/incident_features.parquet...\n",
      "\n",
      "Feature Engineering Complete...\n",
      "Shape of the final incident level feature dataframe: (567609, 8)\n",
      "\n",
      "First 5 rows of the new dataframe:\n",
      "shape: (5, 8)\n",
      "┌────────────┬────────────┬────────────┬───────────┬───────────┬───────────┬───────────┬───────────┐\n",
      "│ IncidentId ┆ IncidentGr ┆ evidence_c ┆ unique_al ┆ unique_en ┆ unique_de ┆ unique_mi ┆ unique_or │\n",
      "│ ---        ┆ ade        ┆ ount       ┆ ert_count ┆ tity_type ┆ tector_id ┆ tre_techn ┆ g_id_coun │\n",
      "│ i64        ┆ ---        ┆ ---        ┆ ---       ┆ _count    ┆ _count    ┆ iques_cou ┆ t         │\n",
      "│            ┆ str        ┆ u32        ┆ u32       ┆ ---       ┆ ---       ┆ nt        ┆ ---       │\n",
      "│            ┆            ┆            ┆           ┆ u32       ┆ u32       ┆ ---       ┆ u32       │\n",
      "│            ┆            ┆            ┆           ┆           ┆           ┆ u32       ┆           │\n",
      "╞════════════╪════════════╪════════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
      "│ 157856     ┆ FalsePosit ┆ 13         ┆ 2         ┆ 8         ┆ 2         ┆ 2         ┆ 2         │\n",
      "│            ┆ ive        ┆            ┆           ┆           ┆           ┆           ┆           │\n",
      "│ 117743     ┆ BenignPosi ┆ 3          ┆ 1         ┆ 2         ┆ 1         ┆ 1         ┆ 1         │\n",
      "│            ┆ tive       ┆            ┆           ┆           ┆           ┆           ┆           │\n",
      "│ 426600     ┆ FalsePosit ┆ 4          ┆ 1         ┆ 4         ┆ 1         ┆ 1         ┆ 1         │\n",
      "│            ┆ ive        ┆            ┆           ┆           ┆           ┆           ┆           │\n",
      "│ 174297     ┆ TruePositi ┆ 11         ┆ 3         ┆ 3         ┆ 3         ┆ 3         ┆ 1         │\n",
      "│            ┆ ve         ┆            ┆           ┆           ┆           ┆           ┆           │\n",
      "│ 165870     ┆ FalsePosit ┆ 12         ┆ 3         ┆ 6         ┆ 3         ┆ 1         ┆ 2         │\n",
      "│            ┆ ive        ┆            ┆           ┆           ┆           ┆           ┆           │\n",
      "└────────────┴────────────┴────────────┴───────────┴───────────┴───────────┴───────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 3: Joining features to base table...\")\n",
    "incident_base_df = incident_base.collect()\n",
    "incident_features_df = incident_features.collect()\n",
    "\n",
    "final_df = incident_base_df.join(incident_features_df, on='IncidentId', how='left')\n",
    "\n",
    "print(f\"Step 4: Saving processed features to {PROCESSED_FEATURES_PATH}...\")\n",
    "final_df.write_parquet(PROCESSED_FEATURES_PATH)\n",
    "\n",
    "print(\"\\nFeature Engineering Complete...\")\n",
    "print(\"Shape of the final incident level feature dataframe:\", final_df.shape)\n",
    "print(f\"\\nFirst 5 rows of the new dataframe:\")\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ce8a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Advanced Feature Engineering\n",
      "Step 1: Engineering temporal features...\n",
      "Step 2: Engineering categorical aggregations...\n"
     ]
    }
   ],
   "source": [
    "lazy_features_df = pl.scan_parquet(PROCESSED_FEATURES_PATH)\n",
    "lazy_df = pl.scan_parquet(TRAIN_DATA_PATH)\n",
    "\n",
    "print(\"Starting Advanced Feature Engineering\")\n",
    "\n",
    "# 1. Temporal Feature Engineering\n",
    "print(\"Step 1: Engineering temporal features...\")\n",
    "temporal_features = (\n",
    "    lazy_df.with_columns(\n",
    "        pl.col('Timestamp').str.to_datetime()\n",
    "    ).group_by('IncidentId').agg(pl.min('Timestamp').alias('first_evidence_ts'), pl.max('Timestamp').alias('last_evidence_ts')).with_columns(\n",
    "        (pl.col('last_evidence_ts') - pl.col('first_evidence_ts')).dt.total_seconds().alias('incident_duration_seconds') +1\n",
    "    )\n",
    ")\n",
    "\n",
    "# 2. Categorical Aggregation\n",
    "print(\"Step 2: Engineering categorical aggregations...\")\n",
    "top_entity_types = ['Ip', 'User', 'MailMessage', 'Machine', 'File']\n",
    "entity_type_expressions = [\n",
    "    pl.col('EntityType').filter(pl.col('EntityType') == entity).count().alias(f'entity_{entity}_count')\n",
    "    for entity in top_entity_types\n",
    "]\n",
    "\n",
    "top_categories = ['InitialAccess', 'Exfiltration', 'SuspiciousActivity', 'CommandAndControl', 'Impact']\n",
    "category_expressions = [\n",
    "    pl.col('Category').filter(pl.col('Category') == cat).count().alias(f'category_{cat}_count')\n",
    "    for cat in top_categories\n",
    "]\n",
    "\n",
    "categorical_features = lazy_df.group_by('IncidentId').agg(*entity_type_expressions, *category_expressions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c753cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Joining all feature sets...\n"
     ]
    },
    {
     "ename": "ComputeError",
     "evalue": "`strptime` / `to_datetime` was called with no format and no time zone, but a time zone is part of the data.\n\nThis was previously allowed but led to unpredictable and erroneous results. Give a format string, set a time zone or perform the operation eagerly on a Series instead of on an Expr.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mComputeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStep 3: Joining all feature sets...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Let's execute and join the new features\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m temporal_features_df = \u001b[43mtemporal_features\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m categorical_features_df = categorical_features.collect()\n\u001b[32m      6\u001b[39m base_features_df = lazy_features_df.collect() \u001b[38;5;66;03m# Load our first feature set\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\Micosoft Security Indicent\\.venv\\Lib\\site-packages\\polars\\_utils\\deprecation.py:97\u001b[39m, in \u001b[36mdeprecate_streaming_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     93\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mengine\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33min-memory\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mstreaming\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\Micosoft Security Indicent\\.venv\\Lib\\site-packages\\polars\\lazyframe\\opt_flags.py:330\u001b[39m, in \u001b[36mforward_old_opt_flags.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m         optflags = cb(optflags, kwargs.pop(key))  \u001b[38;5;66;03m# type: ignore[no-untyped-call,unused-ignore]\u001b[39;00m\n\u001b[32m    329\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33moptimizations\u001b[39m\u001b[33m\"\u001b[39m] = optflags\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\Micosoft Security Indicent\\.venv\\Lib\\site-packages\\polars\\lazyframe\\frame.py:2407\u001b[39m, in \u001b[36mLazyFrame.collect\u001b[39m\u001b[34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, engine, background, optimizations, **_kwargs)\u001b[39m\n\u001b[32m   2405\u001b[39m \u001b[38;5;66;03m# Only for testing purposes\u001b[39;00m\n\u001b[32m   2406\u001b[39m callback = _kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mpost_opt_callback\u001b[39m\u001b[33m\"\u001b[39m, callback)\n\u001b[32m-> \u001b[39m\u001b[32m2407\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(\u001b[43mldf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mComputeError\u001b[39m: `strptime` / `to_datetime` was called with no format and no time zone, but a time zone is part of the data.\n\nThis was previously allowed but led to unpredictable and erroneous results. Give a format string, set a time zone or perform the operation eagerly on a Series instead of on an Expr."
     ]
    }
   ],
   "source": [
    "# 3. Join All Features Together ---\n",
    "print(\"Step 3: Joining all feature sets...\")\n",
    "# Let's execute and join the new features\n",
    "temporal_features_df = temporal_features.collect()\n",
    "categorical_features_df = categorical_features.collect()\n",
    "base_features_df = lazy_features_df.collect() # Load our first feature set\n",
    "\n",
    "# Join temporal features\n",
    "final_df_enhanced = base_features_df.join(temporal_features_df.select(['IncidentId', 'incident_duration_seconds']), on='IncidentId', how='left')\n",
    "\n",
    "# Join categorical features\n",
    "final_df_enhanced = final_df_enhanced.join(categorical_features_df, on='IncidentId', how='left')\n",
    "\n",
    "# --- 4. Create Rate Features ---\n",
    "# Now that we have duration and counts, we can create rates\n",
    "print(\"Step 4: Creating rate features...\")\n",
    "final_df_enhanced = final_df_enhanced.with_columns(\n",
    "    (pl.col('evidence_count') / pl.col('incident_duration_seconds')).alias('evidence_rate'),\n",
    "    (pl.col('unique_alert_count') / pl.col('incident_duration_seconds')).alias('alert_rate')\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134b78b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
