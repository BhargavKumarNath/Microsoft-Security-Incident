{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2364df8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6571e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_TRAIN_PATH = '../data/raw/GUIDE_Train.parquet'\n",
    "PROCESSED_INCIDENT_FEATURES_PATH = '../data/processed/incident_features.parquet'\n",
    "DL_PROCESSED_DATA_DIR = Path('../data/processed_dl/')\n",
    "DL_PROCESSED_DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "MAX_SEQ_LENGTH = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ba07de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Phase 4.1: Preparing Data for Sequence Modeling ---\n",
      "Loading raw and processed data...\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Phase 4.1: Preparing Data for Sequence Modeling ---\")\n",
    "\n",
    "#  1. Load Data \n",
    "print(\"Loading raw and processed data...\")\n",
    "raw_df = pl.read_parquet(RAW_TRAIN_PATH)\n",
    "incident_features_df = pl.read_parquet(PROCESSED_INCIDENT_FEATURES_PATH)\n",
    "\n",
    "sequential_features = [\n",
    "    'Category',\n",
    "    'DetectorId',\n",
    "    'EntityType',\n",
    "    'MitreTechniques' \n",
    "]\n",
    "\n",
    "static_features = [\n",
    "    'OrgId',\n",
    "    'evidence_count',\n",
    "    'unique_alert_count',\n",
    "    'incident_duration_seconds',\n",
    "    'evidence_rate',\n",
    "    'alert_rate'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbb77f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vocabularies for sequential features...\n",
      "  Vocabulary for 'Category' has 21 unique tokens.\n",
      "  Vocabulary for 'DetectorId' has 8429 unique tokens.\n",
      "  Vocabulary for 'EntityType' has 34 unique tokens.\n",
      "  Vocabulary for 'MitreTechniques' has 1195 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating vocabularies for sequential features...\")\n",
    "vocabularies = {}\n",
    "for col in sequential_features:\n",
    "    unique_vals = raw_df[col].fill_null('[NULL]').unique().to_list()\n",
    "    vocab = {val: i + 1 for i, val in enumerate(unique_vals)}\n",
    "    vocab['[PAD]'] = 0  # Padding token\n",
    "    vocabularies[col] = vocab\n",
    "    print(f\"  Vocabulary for '{col}' has {len(vocab)} unique tokens.\")\n",
    "\n",
    "with open(DL_PROCESSED_DATA_DIR / 'vocabularies.pkl', 'wb') as f:\n",
    "    pickle.dump(vocabularies, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59516c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouping by IncidentId and creating tokenized sequences...\n",
      "Tokenization complete.\n",
      "Shape of sequential data: (466151, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Grouping by IncidentId and creating tokenized sequences...\")\n",
    "\n",
    "def tokenize_and_pad_list(values_list: list, vocab: dict) -> list:\n",
    "    tokens = [vocab.get(val, 0) for val in values_list] \n",
    "    tokens = tokens[:MAX_SEQ_LENGTH]\n",
    "    padding_needed = MAX_SEQ_LENGTH - len(tokens)\n",
    "    return tokens + [vocab['[PAD]']] * padding_needed\n",
    "\n",
    "sequential_data = (\n",
    "    raw_df.sort(['IncidentId', 'Timestamp'])\n",
    "          .group_by('IncidentId')\n",
    "          .agg([\n",
    "              pl.col(col).fill_null('[NULL]').alias(f'{col}_list')\n",
    "              for col in sequential_features\n",
    "          ])\n",
    ")\n",
    "\n",
    "for col in sequential_features:\n",
    "    sequential_data = sequential_data.with_columns(\n",
    "        pl.col(f'{col}_list').map_elements(\n",
    "            lambda values_list: tokenize_and_pad_list(values_list, vocabularies[col]),\n",
    "            return_dtype=pl.List(pl.Int32)\n",
    "        ).alias(f'{col}_seq')\n",
    "    ).drop(f'{col}_list')\n",
    "\n",
    "print(\"Tokenization complete.\")\n",
    "print(\"Shape of sequential data:\", sequential_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80052aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preparing static features and labels...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
