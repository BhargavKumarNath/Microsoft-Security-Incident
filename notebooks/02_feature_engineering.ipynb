{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ef44cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aeda14e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Feature Engineering\n",
      "Step 1: Creating base incident table...\n",
      "Step 2: Engineering count and diversity features...\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA_PATH = '../data/raw/GUIDE_Train.parquet'\n",
    "PROCESSED_FEATURES_PATH = '../data/processed/incident_features.parquet'\n",
    "\n",
    "print(\"Starting Feature Engineering\")\n",
    "\n",
    "# Load data\n",
    "lazy_df = pl.scan_parquet(TRAIN_DATA_PATH)\n",
    "\n",
    "# 3. Create base dataframe for incidents\n",
    "print(\"Step 1: Creating base incident table...\")\n",
    "incident_base = (\n",
    "    lazy_df.select(['IncidentId', 'IncidentGrade']).drop_nulls().unique(subset=['IncidentId', 'IncidentGrade'])\n",
    ")\n",
    "\n",
    "# 4. Engineer Count and Diversity Features\n",
    "print(\"Step 2: Engineering count and diversity features...\")\n",
    "feature_expressions = [\n",
    "    pl.len().alias('evidence_count'),\n",
    "    pl.n_unique('AlertId').alias('unique_alert_count'),\n",
    "    pl.n_unique('EntityType').alias('unique_entity_type_count'),\n",
    "    pl.n_unique('DetectorId').alias('unique_detector_id_count'),\n",
    "    pl.n_unique('MitreTechniques').alias('unique_mitre_techniques_count'),\n",
    "    pl.n_unique('OrgId').alias('unique_org_id_count'),\n",
    "]\n",
    "\n",
    "incident_features = lazy_df.group_by('IncidentId').agg(feature_expressions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4330186e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Joining features to base table...\n",
      "Step 4: Saving processed features to ../data/processed/incident_features.parquet...\n",
      "\n",
      "Feature Engineering Complete...\n",
      "Shape of the final incident level feature dataframe: (567609, 8)\n",
      "\n",
      "First 5 rows of the new dataframe:\n",
      "shape: (5, 8)\n",
      "┌────────────┬────────────┬────────────┬───────────┬───────────┬───────────┬───────────┬───────────┐\n",
      "│ IncidentId ┆ IncidentGr ┆ evidence_c ┆ unique_al ┆ unique_en ┆ unique_de ┆ unique_mi ┆ unique_or │\n",
      "│ ---        ┆ ade        ┆ ount       ┆ ert_count ┆ tity_type ┆ tector_id ┆ tre_techn ┆ g_id_coun │\n",
      "│ i64        ┆ ---        ┆ ---        ┆ ---       ┆ _count    ┆ _count    ┆ iques_cou ┆ t         │\n",
      "│            ┆ str        ┆ u32        ┆ u32       ┆ ---       ┆ ---       ┆ nt        ┆ ---       │\n",
      "│            ┆            ┆            ┆           ┆ u32       ┆ u32       ┆ ---       ┆ u32       │\n",
      "│            ┆            ┆            ┆           ┆           ┆           ┆ u32       ┆           │\n",
      "╞════════════╪════════════╪════════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
      "│ 305059     ┆ BenignPosi ┆ 5          ┆ 1         ┆ 4         ┆ 1         ┆ 1         ┆ 1         │\n",
      "│            ┆ tive       ┆            ┆           ┆           ┆           ┆           ┆           │\n",
      "│ 6697       ┆ BenignPosi ┆ 197        ┆ 5         ┆ 7         ┆ 4         ┆ 3         ┆ 5         │\n",
      "│            ┆ tive       ┆            ┆           ┆           ┆           ┆           ┆           │\n",
      "│ 157021     ┆ FalsePosit ┆ 7          ┆ 4         ┆ 5         ┆ 2         ┆ 2         ┆ 2         │\n",
      "│            ┆ ive        ┆            ┆           ┆           ┆           ┆           ┆           │\n",
      "│ 563406     ┆ BenignPosi ┆ 2          ┆ 1         ┆ 2         ┆ 1         ┆ 1         ┆ 1         │\n",
      "│            ┆ tive       ┆            ┆           ┆           ┆           ┆           ┆           │\n",
      "│ 419040     ┆ FalsePosit ┆ 4          ┆ 1         ┆ 4         ┆ 1         ┆ 1         ┆ 1         │\n",
      "│            ┆ ive        ┆            ┆           ┆           ┆           ┆           ┆           │\n",
      "└────────────┴────────────┴────────────┴───────────┴───────────┴───────────┴───────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 3: Joining features to base table...\")\n",
    "incident_base_df = incident_base.collect()\n",
    "incident_features_df = incident_features.collect()\n",
    "\n",
    "final_df = incident_base_df.join(incident_features_df, on='IncidentId', how='left')\n",
    "\n",
    "print(f\"Step 4: Saving processed features to {PROCESSED_FEATURES_PATH}...\")\n",
    "final_df.write_parquet(PROCESSED_FEATURES_PATH)\n",
    "\n",
    "print(\"\\nFeature Engineering Complete...\")\n",
    "print(\"Shape of the final incident level feature dataframe:\", final_df.shape)\n",
    "print(f\"\\nFirst 5 rows of the new dataframe:\")\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02ce8a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Advanced Feature Engineering\n",
      "Step 1: Engineering temporal features...\n",
      "Step 2: Engineering categorical aggregations...\n"
     ]
    }
   ],
   "source": [
    "lazy_features_df = pl.scan_parquet(PROCESSED_FEATURES_PATH)\n",
    "lazy_df = pl.scan_parquet(TRAIN_DATA_PATH)\n",
    "\n",
    "print(\"Starting Advanced Feature Engineering\")\n",
    "\n",
    "# 1. Temporal Feature Engineering\n",
    "print(\"Step 1: Engineering temporal features...\")\n",
    "temporal_features = (\n",
    "    lazy_df.with_columns(\n",
    "        pl.col(\"Timestamp\").str.strptime(pl.Datetime, \"%Y-%m-%dT%H:%M:%S%.3fZ\")\n",
    "    )\n",
    "    .group_by(\"IncidentId\")\n",
    "    .agg(\n",
    "        pl.min(\"Timestamp\").alias(\"first_evidence_ts\"),\n",
    "        pl.max(\"Timestamp\").alias(\"last_evidence_ts\"),\n",
    "    )\n",
    "    .with_columns(\n",
    "        (\n",
    "            (pl.col(\"last_evidence_ts\") - pl.col(\"first_evidence_ts\"))\n",
    "            .dt.total_seconds()\n",
    "            + 1\n",
    "        ).alias(\"incident_duration_seconds\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# 2. Categorical Aggregation\n",
    "print(\"Step 2: Engineering categorical aggregations...\")\n",
    "top_entity_types = ['Ip', 'User', 'MailMessage', 'Machine', 'File']\n",
    "entity_type_expressions = [\n",
    "    pl.col('EntityType').filter(pl.col('EntityType') == entity).count().alias(f'entity_{entity}_count')\n",
    "    for entity in top_entity_types\n",
    "]\n",
    "\n",
    "top_categories = ['InitialAccess', 'Exfiltration', 'SuspiciousActivity', 'CommandAndControl', 'Impact']\n",
    "category_expressions = [\n",
    "    pl.col('Category').filter(pl.col('Category') == cat).count().alias(f'category_{cat}_count')\n",
    "    for cat in top_categories\n",
    "]\n",
    "\n",
    "categorical_features = lazy_df.group_by('IncidentId').agg(*entity_type_expressions, *category_expressions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c753cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Joining all feature sets...\n",
      "Step 4: Creating rate features...\n"
     ]
    }
   ],
   "source": [
    "# 3. Join All Features Together \n",
    "print(\"Step 3: Joining all feature sets...\")\n",
    "temporal_features_df = temporal_features.collect()\n",
    "\n",
    "categorical_features_df = categorical_features.collect()\n",
    "base_features_df = lazy_features_df.collect() \n",
    "\n",
    "final_df_enhanced = base_features_df.join(temporal_features_df.select(['IncidentId', 'incident_duration_seconds']), on='IncidentId', how='left')\n",
    "\n",
    "final_df_enhanced = final_df_enhanced.join(categorical_features_df, on='IncidentId', how='left')\n",
    "\n",
    "# 4. Create Rate Features\n",
    "print(\"Step 4: Creating rate features...\")\n",
    "final_df_enhanced = final_df_enhanced.with_columns(\n",
    "    (pl.col('evidence_count') / pl.col('incident_duration_seconds')).alias('evidence_rate'),\n",
    "    (pl.col('unique_alert_count') / pl.col('incident_duration_seconds')).alias('alert_rate')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "681a71c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: Save the ENHANCED features to ../data/processed/incident_features.parquet...\n",
      "\n",
      "Advanced Feature Engineering Complete\n",
      "Shape of the final enhanced feature dataframe: (567609, 21)\n",
      "\n",
      "First 5 rows of the enhanced dataframe:\n",
      "shape: (5, 21)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
      "│ IncidentI ┆ IncidentG ┆ evidence_ ┆ unique_al ┆ … ┆ category_ ┆ category_ ┆ evidence_ ┆ alert_ra │\n",
      "│ d         ┆ rade      ┆ count     ┆ ert_count ┆   ┆ CommandAn ┆ Impact_co ┆ rate      ┆ te       │\n",
      "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ dControl_ ┆ unt       ┆ ---       ┆ ---      │\n",
      "│ i64       ┆ str       ┆ u32       ┆ u32       ┆   ┆ cou…      ┆ ---       ┆ f64       ┆ f64      │\n",
      "│           ┆           ┆           ┆           ┆   ┆ ---       ┆ u32       ┆           ┆          │\n",
      "│           ┆           ┆           ┆           ┆   ┆ u32       ┆           ┆           ┆          │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
      "│ 305059    ┆ BenignPos ┆ 5         ┆ 1         ┆ … ┆ 5         ┆ 0         ┆ 5.0       ┆ 1.0      │\n",
      "│           ┆ itive     ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 6697      ┆ BenignPos ┆ 197       ┆ 5         ┆ … ┆ 0         ┆ 0         ┆ 0.000218  ┆ 0.000006 │\n",
      "│           ┆ itive     ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 157021    ┆ FalsePosi ┆ 7         ┆ 4         ┆ … ┆ 0         ┆ 3         ┆ 0.000005  ┆ 0.000003 │\n",
      "│           ┆ tive      ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 563406    ┆ BenignPos ┆ 2         ┆ 1         ┆ … ┆ 0         ┆ 0         ┆ 2.0       ┆ 1.0      │\n",
      "│           ┆ itive     ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 419040    ┆ FalsePosi ┆ 4         ┆ 1         ┆ … ┆ 0         ┆ 0         ┆ 4.0       ┆ 1.0      │\n",
      "│           ┆ tive      ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "print(f\"Step 5: Save the ENHANCED features to {PROCESSED_FEATURES_PATH}...\")\n",
    "final_df_enhanced.write_parquet(PROCESSED_FEATURES_PATH)\n",
    "\n",
    "print(\"\\nAdvanced Feature Engineering Complete\")\n",
    "print(\"Shape of the final enhanced feature dataframe:\", final_df_enhanced.shape)\n",
    "print(\"\\nFirst 5 rows of the enhanced dataframe:\")\n",
    "print(final_df_enhanced.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac183dca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
