{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c8fd6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d62b19cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Model Training\n",
      "Loaded feature matrix with shape: (567609, 22)\n"
     ]
    }
   ],
   "source": [
    "PROCESSED_FEATURES_PATH = '../data/processed/incident_features.parquet'\n",
    "df = pl.read_parquet(PROCESSED_FEATURES_PATH)\n",
    "\n",
    "raw_lazy_df = pl.scan_parquet('../data/raw/GUIDE_Train.parquet')\n",
    "org_id_map = raw_lazy_df.select(['IncidentId', 'OrgId']).unique(subset=['IncidentId']).collect()\n",
    "df = df.join(org_id_map, on='IncidentId', how='left')\n",
    "\n",
    "df = df.fill_null(0)\n",
    "\n",
    "print(\"Starting Model Training\")\n",
    "print(f\"Loaded feature matrix with shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3e6ff8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IncidentId',\n",
       " 'IncidentGrade',\n",
       " 'evidence_count',\n",
       " 'unique_alert_count',\n",
       " 'unique_entity_type_count',\n",
       " 'unique_detector_id_count',\n",
       " 'unique_mitre_techniques_count',\n",
       " 'unique_org_id_count',\n",
       " 'incident_duration_seconds',\n",
       " 'entity_Ip_count',\n",
       " 'entity_User_count',\n",
       " 'entity_MailMessage_count',\n",
       " 'entity_Machine_count',\n",
       " 'entity_File_count',\n",
       " 'category_InitialAccess_count',\n",
       " 'category_Exfiltration_count',\n",
       " 'category_SuspiciousActivity_count',\n",
       " 'category_CommandAndControl_count',\n",
       " 'category_Impact_count',\n",
       " 'evidence_rate',\n",
       " 'alert_rate',\n",
       " 'OrgId']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f8896f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target classes encoded: [(0, 'BenignPositive'), (1, 'FalsePositive'), (2, 'TruePositive')]\n"
     ]
    }
   ],
   "source": [
    "df_pd = df.to_pandas()\n",
    "\n",
    "TARGET = 'IncidentGrade'\n",
    "features = [col for col in df_pd.columns if col not in ['IncidentId', TARGET, 'first_evidence_ts', 'last_evidence_ts']]\n",
    "X = df_pd[features]\n",
    "y_raw = df_pd[TARGET]\n",
    "groups = df_pd['OrgId']\n",
    "\n",
    "# Encode the string labels into integers\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y_raw)\n",
    "class_names = le.classes_\n",
    "print(f\"Target classes encoded: {list(zip(range(len(class_names)), class_names))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d17d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up cross validation\n",
    "N_SPLITS = 5\n",
    "cv = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "class_weight_dict = dict(zip(np.unique(y), class_weights))\n",
    "\n",
    "lgbm = lgb.LGBMClassifier(random_state=42,               \n",
    "                          n_jobs=-1,\n",
    "                          objective='multiclass',\n",
    "                          class_weight=class_weight_dict,\n",
    "                          num_leaves=100,\n",
    "                          learning_rate=0.05,\n",
    "                          feature_fraction=0.8,\n",
    "                          bagging_fraction=0.8,\n",
    "                          bagging_freq=5,\n",
    "                          min_child_samples=100,\n",
    "                          reg_alpha=0.1,\n",
    "                          reg_lambda=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d9ff642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting 5-Fold Stratified Group Cross Validation...\n",
      "=== Fold 1/5 ===\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009256 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3973\n",
      "[LightGBM] [Info] Number of data points in the train set: 476009, number of used features: 20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Start training from score -1.104123\n",
      "[LightGBM] [Info] Start training from score -1.063905\n",
      "[LightGBM] [Info] Start training from score -1.128887\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Macro F1 0.36108 | Macro Precision: 0.36748 | Macro Recall: 0.38264\n",
      "=== Fold 2/5 ===\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008847 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3972\n",
      "[LightGBM] [Info] Number of data points in the train set: 493613, number of used features: 20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Start training from score -1.119164\n",
      "[LightGBM] [Info] Start training from score -1.096699\n",
      "[LightGBM] [Info] Start training from score -1.080353\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Macro F1 0.44829 | Macro Precision: 0.47859 | Macro Recall: 0.46305\n",
      "=== Fold 3/5 ===\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007370 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3974\n",
      "[LightGBM] [Info] Number of data points in the train set: 396498, number of used features: 20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Start training from score -1.025959\n",
      "[LightGBM] [Info] Start training from score -1.125316\n",
      "[LightGBM] [Info] Start training from score -1.148861\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Macro F1 0.44722 | Macro Precision: 0.56421 | Macro Recall: 0.47266\n",
      "=== Fold 4/5 ===\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008167 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3899\n",
      "[LightGBM] [Info] Number of data points in the train set: 482961, number of used features: 20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Start training from score -1.088927\n",
      "[LightGBM] [Info] Start training from score -1.099550\n",
      "[LightGBM] [Info] Start training from score -1.107446\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Macro F1 0.45263 | Macro Precision: 0.47177 | Macro Recall: 0.45563\n",
      "=== Fold 5/5 ===\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008480 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3962\n",
      "[LightGBM] [Info] Number of data points in the train set: 421355, number of used features: 20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Start training from score -1.150102\n",
      "[LightGBM] [Info] Start training from score -1.115116\n",
      "[LightGBM] [Info] Start training from score -1.034178\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Macro F1 0.53179 | Macro Precision: 0.55087 | Macro Recall: 0.53799\n"
     ]
    }
   ],
   "source": [
    "oof_preds = np.zeros((len(df_pd), len(class_names)))\n",
    "f1_scores, precision_scores, recall_scores = [], [], []\n",
    "\n",
    "print(f\"\\nStarting {N_SPLITS}-Fold Stratified Group Cross Validation...\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X, y, groups)):\n",
    "    print(f\"=== Fold {fold+1}/{N_SPLITS} ===\")\n",
    "\n",
    "    X_train, y_train = X.iloc[train_idx], y[train_idx]\n",
    "    X_val, y_val = X.iloc[val_idx], y[val_idx]\n",
    "\n",
    "    # Train it\n",
    "    lgbm.fit(X_train, y_train,\n",
    "             eval_set=[(X_val, y_val)],\n",
    "             eval_metric='multi_logloss',\n",
    "             callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "    \n",
    "    # Predict on Validation set\n",
    "    val_preds = lgbm.predict(X_val)\n",
    "    val_preds_proba = lgbm.predict_proba(X_val)\n",
    "    oof_preds[val_idx] = val_preds_proba\n",
    "\n",
    "    # Calculate and store metrics\n",
    "    f1 = f1_score(y_val, val_preds, average='macro')\n",
    "    precision = precision_score(y_val, val_preds, average='macro')\n",
    "    recall = recall_score(y_val, val_preds, average='macro')\n",
    "\n",
    "    f1_scores.append(f1)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "\n",
    "    print(f\"Macro F1 {f1:.5f} | Macro Precision: {precision:.5f} | Macro Recall: {recall:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c215f776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overcall Cross Validation Results\n",
      "Mean Macro F1: 0.44820 ± 0.05403\n",
      "Mean Macro Precision: 0.48658 ± 0.07018\n",
      "Mean Macro Recall: 0.46240 ± 0.049451\n"
     ]
    }
   ],
   "source": [
    "print(\"Overcall Cross Validation Results\")\n",
    "print(f\"Mean Macro F1: {np.mean(f1_scores):.5f} ± {np.std(f1_scores):.5f}\")\n",
    "print(f\"Mean Macro Precision: {np.mean(precision_scores):.5f} ± {np.std(precision_scores):.5f}\")\n",
    "print(f\"Mean Macro Recall: {np.mean(recall_scores):.5f} ± {np.std(recall_scores):.5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27deeee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "BenignPositive       0.55      0.59      0.57    260495\n",
      " FalsePositive       0.61      0.49      0.54    177771\n",
      "  TruePositive       0.26      0.29      0.28    129343\n",
      "\n",
      "      accuracy                           0.49    567609\n",
      "     macro avg       0.47      0.46      0.46    567609\n",
      "  weighted avg       0.50      0.49      0.50    567609\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "oof_pred_classes = np.argmax(oof_preds, axis=1)\n",
    "print(classification_report(y, oof_pred_classes, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e323196e",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80340a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd6d362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights for XGBoost\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "\n",
    "xgb_classifier = xgb.XGBClassifier(\n",
    "    objective='multi:softprob',  \n",
    "    num_class=len(class_names),  \n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    n_estimators=500,           \n",
    "    max_depth=8,                \n",
    "    learning_rate=0.05,         \n",
    "    reg_alpha=0.1,              \n",
    "    reg_lambda=0.1,             \n",
    "    gamma=0.1,                  \n",
    "    \n",
    "    subsample=0.8,              \n",
    "    colsample_bytree=0.8,       \n",
    "    colsample_bylevel=0.8,      \n",
    "    \n",
    "    min_child_weight=5,         \n",
    "    max_delta_step=1,           \n",
    "    \n",
    "    eval_metric='mlogloss'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aaa5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting 5-Fold XGBoost Cross Validation...\n",
      "=== Fold 1/5 ===\n",
      "Macro F1: 0.43787 | Macro Precision: 0.44744 | Macro Recall: 0.44152\n",
      "=== Fold 2/5 ===\n",
      "Macro F1: 0.42636 | Macro Precision: 0.44785 | Macro Recall: 0.44371\n",
      "=== Fold 3/5 ===\n",
      "Macro F1: 0.46694 | Macro Precision: 0.47221 | Macro Recall: 0.46730\n",
      "=== Fold 4/5 ===\n",
      "Macro F1: 0.50541 | Macro Precision: 0.51983 | Macro Recall: 0.51048\n",
      "=== Fold 5/5 ===\n",
      "Macro F1: 0.45478 | Macro Precision: 0.50010 | Macro Recall: 0.45974\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nStarting {N_SPLITS}-Fold XGBoost Cross Validation...\")\n",
    "\n",
    "oof_preds_xgb = np.zeros((len(df_pd), len(class_names)))\n",
    "f1_scores_xgb, precision_scores_xgb, recall_scores_xgb = [], [], []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X, y, groups)):\n",
    "    print(f\"=== Fold {fold+1}/{N_SPLITS} ===\")\n",
    "\n",
    "    X_train, y_train = X.iloc[train_idx], y[train_idx]\n",
    "    X_val, y_val = X.iloc[val_idx], y[val_idx]\n",
    "    \n",
    "    sample_weights = np.array([class_weights[label] for label in y_train])\n",
    "    \n",
    "    xgb_fold = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        num_class=len(class_names),\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        n_estimators=500,\n",
    "        max_depth=8,\n",
    "        learning_rate=0.05,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=0.1,\n",
    "        gamma=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        colsample_bylevel=0.8,\n",
    "        min_child_weight=5,\n",
    "        max_delta_step=1,\n",
    "        eval_metric='mlogloss',\n",
    "        early_stopping_rounds=50\n",
    "    )\n",
    "    \n",
    "    xgb_fold.fit(\n",
    "        X_train, y_train,\n",
    "        sample_weight=sample_weights,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    val_preds = xgb_fold.predict(X_val)\n",
    "    val_preds_proba = xgb_fold.predict_proba(X_val)\n",
    "    oof_preds_xgb[val_idx] = val_preds_proba\n",
    "\n",
    "    f1 = f1_score(y_val, val_preds, average='macro')\n",
    "    precision = precision_score(y_val, val_preds, average='macro')\n",
    "    recall = recall_score(y_val, val_preds, average='macro')\n",
    "\n",
    "    f1_scores_xgb.append(f1)\n",
    "    precision_scores_xgb.append(precision)\n",
    "    recall_scores_xgb.append(recall)\n",
    "\n",
    "    print(f\"Macro F1: {f1:.5f} | Macro Precision: {precision:.5f} | Macro Recall: {recall:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e86d64b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== XGBoost Cross-Validation Results ===\n",
      "Mean Macro F1: 0.45827 ± 0.02736\n",
      "Mean Macro Precision: 0.47749 ± 0.02868\n",
      "Mean Macro Recall: 0.46455 ± 0.02492\n",
      "\n",
      "=== XGBoost Classification Report ===\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "BenignPositive       0.60      0.54      0.57    260495\n",
      " FalsePositive       0.54      0.44      0.49    177771\n",
      "  TruePositive       0.29      0.43      0.34    129343\n",
      "\n",
      "      accuracy                           0.48    567609\n",
      "     macro avg       0.48      0.47      0.47    567609\n",
      "  weighted avg       0.51      0.48      0.49    567609\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n=== XGBoost Cross-Validation Results ===\")\n",
    "print(f\"Mean Macro F1: {np.mean(f1_scores_xgb):.5f} ± {np.std(f1_scores_xgb):.5f}\")\n",
    "print(f\"Mean Macro Precision: {np.mean(precision_scores_xgb):.5f} ± {np.std(precision_scores_xgb):.5f}\")\n",
    "print(f\"Mean Macro Recall: {np.mean(recall_scores_xgb):.5f} ± {np.std(recall_scores_xgb):.5f}\")\n",
    "\n",
    "# Classification report\n",
    "oof_pred_classes_xgb = np.argmax(oof_preds_xgb, axis=1)\n",
    "print(f\"\\n=== XGBoost Classification Report ===\")\n",
    "print(classification_report(y, oof_pred_classes_xgb, target_names=class_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a5a62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Feature Importance Analysis ===\n",
      "Top 10 Most Important Features:\n",
      "                              feature  importance\n",
      "19                              OrgId    0.153517\n",
      "12       category_InitialAccess_count    0.104378\n",
      "0                      evidence_count    0.079487\n",
      "11                  entity_File_count    0.066091\n",
      "2            unique_entity_type_count    0.059980\n",
      "14  category_SuspiciousActivity_count    0.059542\n",
      "13        category_Exfiltration_count    0.055212\n",
      "5                 unique_org_id_count    0.055055\n",
      "17                      evidence_rate    0.053357\n",
      "7                     entity_Ip_count    0.051994\n"
     ]
    }
   ],
   "source": [
    "# Feature importance analysis\n",
    "print(f\"\\n=== Feature Importance Analysis ===\")\n",
    "final_xgb = xgb.XGBClassifier(\n",
    "    objective='multi:softprob',\n",
    "    num_class=len(class_names),\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    n_estimators=200,  \n",
    "    max_depth=8,\n",
    "    learning_rate=0.05,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=0.1,\n",
    "    gamma=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    colsample_bylevel=0.8,\n",
    "    min_child_weight=5,\n",
    "    max_delta_step=1,\n",
    "    eval_metric='mlogloss'\n",
    ")\n",
    "\n",
    "final_sample_weights = np.array([class_weights[label] for label in y])\n",
    "final_xgb.fit(X, y, sample_weight=final_sample_weights)\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': final_xgb.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ca8d372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Ensemble Results ===\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "BenignPositive       0.60      0.55      0.57    260495\n",
      " FalsePositive       0.60      0.49      0.54    177771\n",
      "  TruePositive       0.29      0.41      0.34    129343\n",
      "\n",
      "      accuracy                           0.50    567609\n",
      "     macro avg       0.50      0.48      0.48    567609\n",
      "  weighted avg       0.53      0.50      0.51    567609\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ensemble_preds = 0.5 * oof_preds + 0.5 * oof_preds_xgb\n",
    "ensemble_classes = np.argmax(ensemble_preds, axis=1)\n",
    "\n",
    "print(\"=== Ensemble Results ===\")\n",
    "print(classification_report(y, ensemble_classes, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1bff74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
