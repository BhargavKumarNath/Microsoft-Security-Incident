{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcd0131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "import psutil\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import VarianceThreshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22e6dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_FEATURES_PATH = '../data/processed/incident_features.parquet'\n",
    "RAW_FEATURES_PATH = '../data/raw/GUIDE_Train.parquet'\n",
    "TARGET = 'IncidentGrade'\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "MAX_FEATURES_VARIANCE = 0.01\n",
    "\n",
    "OUT_DIR = Path('models')\n",
    "LGB_DIR = OUT_DIR / 'lightgbm'\n",
    "XGB_DIR = OUT_DIR / 'xgboost'\n",
    "ENS_DIR = OUT_DIR / 'ensemble'\n",
    "for d in (LGB_DIR, XGB_DIR, ENS_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Memory monitoring\n",
    "def print_memory_usage(stage=\"\"):\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "    memory_percent = process.memory_percent()\n",
    "    print(f\"[{stage}] Memory: {memory_mb:.1f} MB ({memory_percent:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b245ac",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29e67a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df: pd.DataFrame, verbose: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"Aggressively downcast numeric columns to save memory.\"\"\"\n",
    "    start_mem = df.memory_usage(deep=True).sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if pd.api.types.is_numeric_dtype(col_type):\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if pd.api.types.is_integer_dtype(col_type):\n",
    "                if c_min >= np.iinfo(np.int8).min and c_max <= np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min >= np.iinfo(np.int16).min and c_max <= np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min >= np.iinfo(np.int32).min and c_max <= np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                # Always use float32 for better memory and speed\n",
    "                df[col] = pd.to_numeric(df[col], downcast='float').astype(np.float32)\n",
    "        elif pd.api.types.is_object_dtype(col_type):\n",
    "            # Convert to category immediately for memory savings\n",
    "            nunique = df[col].nunique(dropna=False)\n",
    "            if nunique < len(df) * 0.5:  # If less than 50% unique\n",
    "                df[col] = df[col].astype('category')\n",
    "    \n",
    "    end_mem = df.memory_usage(deep=True).sum() / 1024 ** 2\n",
    "    if verbose:\n",
    "        print(f\"Memory usage: {start_mem:.2f} MB -> {end_mem:.2f} MB ({100 * (start_mem - end_mem) / start_mem:.1f}% reduction)\")\n",
    "    return df\n",
    "\n",
    "def save_pickle(obj, path: Path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d987d3",
   "metadata": {},
   "source": [
    "# SECTION 1: PREPROCESSING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6de45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SECTION 1: PREPROCESSING (MEMORY-OPTIMIZED)\n",
      "[Start] Memory: 214.5 MB (1.4%)\n",
      "Loading parquet files with Polars...\n",
      "Features shape: (567609, 21)\n",
      "Raw data shape: (9516837, 6)\n",
      "Joining and preprocessing with Polars...\n",
      "Joined shape: (14857136, 26)\n",
      "High-cardinality: OrgId (5746 unique). Creating frequency feature.\n",
      "High-cardinality: DetectorId (8149 unique). Creating frequency feature.\n",
      "High-cardinality: AlertTitle (81172 unique). Creating frequency feature.\n",
      "[After Polars preprocessing] Memory: 5214.7 MB (33.3%)\n"
     ]
    }
   ],
   "source": [
    "print('SECTION 1: PREPROCESSING (MEMORY-OPTIMIZED)')\n",
    "\n",
    "print_memory_usage(\"Start\")\n",
    "\n",
    "print('Loading parquet files with Polars...')\n",
    "df_features = pl.read_parquet(PROCESSED_FEATURES_PATH)\n",
    "print(f\"Features shape: {df_features.shape}\")\n",
    "\n",
    "raw_df = pl.read_parquet(RAW_FEATURES_PATH).select([\n",
    "    'IncidentId', 'OrgId', 'DetectorId', 'AlertTitle', 'EntityType', 'Category'\n",
    "])\n",
    "print(f\"Raw data shape: {raw_df.shape}\")\n",
    "\n",
    "print('Joining and preprocessing with Polars...')\n",
    "categorical_cols = ['OrgId', 'DetectorId', 'AlertTitle', 'EntityType', 'Category']\n",
    "for col in categorical_cols:\n",
    "    if col in raw_df.columns:\n",
    "        raw_df = raw_df.with_columns(pl.col(col).fill_null('missing'))\n",
    "\n",
    "df = df_features.join(raw_df, on='IncidentId', how='left')\n",
    "print(f\"Joined shape: {df.shape}\")\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col not in df.columns:\n",
    "        continue\n",
    "    \n",
    "    # Check cardinality\n",
    "    nuni = df.select(pl.col(col).n_unique()).item()\n",
    "    if nuni > 1000:\n",
    "        print(f'High-cardinality: {col} ({nuni} unique). Creating frequency feature.')\n",
    "        \n",
    "        # Create frequency feature\n",
    "        freq_expr = (\n",
    "            df.select(col)\n",
    "            .group_by(col)\n",
    "            .agg(pl.len().alias('freq'))\n",
    "        )\n",
    "        df = df.join(freq_expr, on=col, how='left')\n",
    "        df = df.rename({'freq': f'{col}_freq'})\n",
    "        \n",
    "        # Keep only top 100 categories\n",
    "        top_cats = (\n",
    "            df.select(col)\n",
    "            .group_by(col)\n",
    "            .agg(pl.len().alias('count'))\n",
    "            .sort('count', descending=True)\n",
    "            .head(100)\n",
    "            .select(col)\n",
    "            .to_series()\n",
    "            .to_list()\n",
    "        )\n",
    "        \n",
    "        df = df.with_columns(\n",
    "            pl.when(pl.col(col).is_in(top_cats))\n",
    "            .then(pl.col(col))\n",
    "            .otherwise(pl.lit('other'))\n",
    "            .alias(col)\n",
    "        )\n",
    "\n",
    "print_memory_usage(\"After Polars preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2eab9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to pandas...\n",
      "[After pandas conversion] Memory: 7937.2 MB (50.7%)\n",
      "Reducing memory usage...\n",
      "Memory usage: 6256.03 MB -> 906.84 MB (85.5% reduction)\n",
      "[After memory optimization] Memory: 8015.7 MB (51.2%)\n",
      "Preparing target...\n",
      "Target classes: 3 -> CategoricalIndex(['BenignPositive', 'FalsePositive', 'TruePositive'], categories=['BenignPositive', 'FalsePositive', 'TruePositive'], ordered=False, dtype='category')\n",
      "Class weights: {np.int64(0): np.float64(0.8168810045592506), np.int64(1): np.float64(1.2439955596036465), np.int64(2): np.float64(1.0288382301900767)}\n"
     ]
    }
   ],
   "source": [
    "print('Converting to pandas...')\n",
    "df = df.to_pandas()\n",
    "\n",
    "print_memory_usage(\"After pandas conversion\")\n",
    "\n",
    "# Aggressive memory reduction\n",
    "print('Reducing memory usage...')\n",
    "df = reduce_mem_usage(df, verbose=True)\n",
    "\n",
    "# Convert remaining categoricals\n",
    "for col in categorical_cols:\n",
    "    if col in df.columns and df[col].dtype != 'category':\n",
    "        df[col] = df[col].astype('category')\n",
    "\n",
    "print_memory_usage(\"After memory optimization\")\n",
    "\n",
    "# Prepare target\n",
    "print('Preparing target...')\n",
    "y, class_names = pd.factorize(df[TARGET])\n",
    "num_classes = len(np.unique(y))\n",
    "print(f'Target classes: {num_classes} -> {class_names}')\n",
    "\n",
    "# Compute class weights more efficiently\n",
    "unique_y, counts_y = np.unique(y, return_counts=True)\n",
    "class_weights = len(y) / (num_classes * counts_y)\n",
    "class_weight_dict = dict(zip(unique_y, class_weights))\n",
    "print('Class weights:', class_weight_dict)\n",
    "\n",
    "# Prepare features\n",
    "FEATURES_DROP = [TARGET, 'IncidentId']\n",
    "X = df.drop(columns=[c for c in FEATURES_DROP if c in df.columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f10f22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features: 22\n",
      "Selected numeric features: 22/22\n",
      "Final features: 27 (numeric: 22, categorical: 5)\n",
      "[After feature selection] Memory: 7641.7 MB (48.8%)\n"
     ]
    }
   ],
   "source": [
    "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f'Numeric features: {len(numeric_cols)}')\n",
    "\n",
    "if len(numeric_cols) > 0:\n",
    "    sample_size = min(10000, len(X))\n",
    "    if len(X) > sample_size:\n",
    "        sample_idx = np.random.choice(len(X), sample_size, replace=False)\n",
    "        X_sample = X.iloc[sample_idx]\n",
    "    else:\n",
    "        X_sample = X\n",
    "    \n",
    "    selector = VarianceThreshold(threshold=MAX_FEATURES_VARIANCE)\n",
    "    selector.fit(X_sample[numeric_cols])\n",
    "    selected_numeric = [c for i, c in enumerate(numeric_cols) if selector.get_support()[i]]\n",
    "    print(f'Selected numeric features: {len(selected_numeric)}/{len(numeric_cols)}')\n",
    "else:\n",
    "    selected_numeric = []\n",
    "\n",
    "# Final feature set\n",
    "CATEGORICAL_FEATURES = [c for c in categorical_cols if c in X.columns]\n",
    "all_selected_features = selected_numeric + CATEGORICAL_FEATURES\n",
    "X_selected = X[all_selected_features].copy()\n",
    "\n",
    "print(f'Final features: {X_selected.shape[1]} (numeric: {len(selected_numeric)}, categorical: {len(CATEGORICAL_FEATURES)})')\n",
    "\n",
    "# Ensure numeric columns are float32\n",
    "for c in selected_numeric:\n",
    "    if X_selected[c].dtype != np.float32:\n",
    "        X_selected[c] = X_selected[c].astype(np.float32)\n",
    "\n",
    "print_memory_usage(\"After feature selection\")\n",
    "\n",
    "# Save feature metadata\n",
    "feature_info = {\n",
    "    'feature_names': list(X_selected.columns),\n",
    "    'categorical_features': CATEGORICAL_FEATURES,\n",
    "    'class_names': list(class_names),\n",
    "    'class_weights': class_weight_dict,\n",
    "}\n",
    "save_pickle(feature_info, OUT_DIR / f'feature_info_{TIMESTAMP}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0580742f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[After cleanup] Memory: 5999.9 MB (38.3%)\n"
     ]
    }
   ],
   "source": [
    "# Clean up memory\n",
    "del df, df_features, raw_df, X\n",
    "gc.collect()\n",
    "print_memory_usage(\"After cleanup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f8b61d",
   "metadata": {},
   "source": [
    "# SECTION 2: LIGHTGBM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b22931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SECTION 2: LIGHTGBM TRAINING (OPTIMIZED)\n"
     ]
    }
   ],
   "source": [
    "print('SECTION 2: LIGHTGBM TRAINING (OPTIMIZED)')\n",
    "\n",
    "cv = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# Pre-allocate OOF array\n",
    "oof_lgb = np.zeros((len(X_selected), num_classes), dtype=np.float32)\n",
    "feature_importances_lgb = pd.DataFrame(index=X_selected.columns)\n",
    "\n",
    "lgb_params = {\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': num_classes,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'n_jobs': 4,  \n",
    "    'learning_rate': 0.1,  \n",
    "    'n_estimators': 1000,  \n",
    "    'num_leaves': 31,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.8,\n",
    "    'subsample_freq': 5,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1,\n",
    "    'min_child_samples': 20,\n",
    "    'min_child_weight': 0.001,\n",
    "    'min_split_gain': 0.0,\n",
    "    'class_weight': 'balanced',  \n",
    "    'verbose': -1,\n",
    "    'force_col_wise': True,  \n",
    "    'max_bin': 255,  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e0deaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LGB folds:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LGB FOLD 1/5\n",
      "[Fold 1 start] Memory: 5780.3 MB (36.9%)\n",
      "Train: (11885708, 27), Val: (2971428, 27)\n",
      "Starting LightGBM training...\n",
      "[50]\tvalid_0's multi_logloss: 0.756767\n",
      "[100]\tvalid_0's multi_logloss: 0.733933\n",
      "[150]\tvalid_0's multi_logloss: 0.720216\n",
      "[200]\tvalid_0's multi_logloss: 0.709568\n",
      "[250]\tvalid_0's multi_logloss: 0.702159\n",
      "[300]\tvalid_0's multi_logloss: 0.696551\n",
      "[350]\tvalid_0's multi_logloss: 0.691461\n",
      "[400]\tvalid_0's multi_logloss: 0.687267\n",
      "[450]\tvalid_0's multi_logloss: 0.683842\n",
      "[500]\tvalid_0's multi_logloss: 0.68089\n",
      "[550]\tvalid_0's multi_logloss: 0.678308\n",
      "[600]\tvalid_0's multi_logloss: 0.675998\n",
      "[650]\tvalid_0's multi_logloss: 0.673934\n",
      "[700]\tvalid_0's multi_logloss: 0.67196\n",
      "[750]\tvalid_0's multi_logloss: 0.670317\n",
      "[800]\tvalid_0's multi_logloss: 0.668782\n",
      "[850]\tvalid_0's multi_logloss: 0.667342\n",
      "[900]\tvalid_0's multi_logloss: 0.665996\n",
      "[950]\tvalid_0's multi_logloss: 0.664731\n",
      "[1000]\tvalid_0's multi_logloss: 0.663623\n",
      "Training completed in: 0:19:39.761328\n",
      "Saved LightGBM fold 1\n",
      "Fold 1 - Macro F1: 0.61955, Weighted F1: 0.61879\n",
      "\n",
      "Top 10 features: ['OrgId', 'OrgId_freq', 'DetectorId', 'incident_duration_seconds', 'evidence_rate', 'AlertTitle', 'evidence_count', 'alert_rate', 'category_InitialAccess_count', 'unique_entity_type_count']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LGB folds:  20%|██        | 1/5 [21:22<1:25:30, 1282.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1 end] Memory: 711.5 MB (4.5%)\n",
      "\n",
      "LGB FOLD 2/5\n",
      "[Fold 2 start] Memory: 881.6 MB (5.6%)\n",
      "Train: (11885709, 27), Val: (2971427, 27)\n",
      "Starting LightGBM training...\n",
      "[50]\tvalid_0's multi_logloss: 0.756348\n",
      "[100]\tvalid_0's multi_logloss: 0.73349\n",
      "[150]\tvalid_0's multi_logloss: 0.719345\n",
      "[200]\tvalid_0's multi_logloss: 0.708931\n",
      "[250]\tvalid_0's multi_logloss: 0.701454\n",
      "[300]\tvalid_0's multi_logloss: 0.695607\n",
      "[350]\tvalid_0's multi_logloss: 0.690707\n",
      "[400]\tvalid_0's multi_logloss: 0.686649\n",
      "[450]\tvalid_0's multi_logloss: 0.683143\n",
      "[500]\tvalid_0's multi_logloss: 0.67995\n",
      "[550]\tvalid_0's multi_logloss: 0.677355\n",
      "[600]\tvalid_0's multi_logloss: 0.674801\n",
      "[650]\tvalid_0's multi_logloss: 0.672727\n",
      "[700]\tvalid_0's multi_logloss: 0.6707\n",
      "[750]\tvalid_0's multi_logloss: 0.669007\n",
      "[800]\tvalid_0's multi_logloss: 0.667504\n",
      "[850]\tvalid_0's multi_logloss: 0.666117\n",
      "[900]\tvalid_0's multi_logloss: 0.664736\n",
      "[950]\tvalid_0's multi_logloss: 0.663622\n",
      "[1000]\tvalid_0's multi_logloss: 0.662503\n",
      "Training completed in: 0:18:29.017962\n",
      "Saved LightGBM fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LGB folds:  40%|████      | 2/5 [41:16<1:01:30, 1230.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 - Macro F1: 0.61951, Weighted F1: 0.61881\n",
      "\n",
      "Top 10 features: ['OrgId', 'OrgId_freq', 'DetectorId', 'incident_duration_seconds', 'AlertTitle', 'evidence_rate', 'evidence_count', 'category_InitialAccess_count', 'alert_rate', 'unique_entity_type_count']\n",
      "[Fold 2 end] Memory: 880.0 MB (5.6%)\n",
      "\n",
      "LGB FOLD 3/5\n",
      "[Fold 3 start] Memory: 1132.7 MB (7.2%)\n",
      "Train: (11885709, 27), Val: (2971427, 27)\n",
      "Starting LightGBM training...\n",
      "[50]\tvalid_0's multi_logloss: 0.756882\n",
      "[100]\tvalid_0's multi_logloss: 0.733314\n",
      "[150]\tvalid_0's multi_logloss: 0.718768\n",
      "[200]\tvalid_0's multi_logloss: 0.708276\n",
      "[250]\tvalid_0's multi_logloss: 0.70143\n",
      "[300]\tvalid_0's multi_logloss: 0.695659\n",
      "[350]\tvalid_0's multi_logloss: 0.69089\n",
      "[400]\tvalid_0's multi_logloss: 0.68701\n",
      "[450]\tvalid_0's multi_logloss: 0.683276\n",
      "[500]\tvalid_0's multi_logloss: 0.680243\n",
      "[550]\tvalid_0's multi_logloss: 0.677489\n",
      "[600]\tvalid_0's multi_logloss: 0.675113\n",
      "[650]\tvalid_0's multi_logloss: 0.673064\n",
      "[700]\tvalid_0's multi_logloss: 0.671148\n",
      "[750]\tvalid_0's multi_logloss: 0.669455\n",
      "[800]\tvalid_0's multi_logloss: 0.667963\n",
      "[850]\tvalid_0's multi_logloss: 0.666503\n",
      "[900]\tvalid_0's multi_logloss: 0.66526\n",
      "[950]\tvalid_0's multi_logloss: 0.663983\n",
      "[1000]\tvalid_0's multi_logloss: 0.662804\n",
      "Training completed in: 0:16:56.897715\n",
      "Saved LightGBM fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LGB folds:  60%|██████    | 3/5 [59:57<39:20, 1180.49s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 - Macro F1: 0.62012, Weighted F1: 0.61943\n",
      "\n",
      "Top 10 features: ['OrgId', 'OrgId_freq', 'DetectorId', 'incident_duration_seconds', 'AlertTitle', 'evidence_rate', 'evidence_count', 'category_InitialAccess_count', 'alert_rate', 'unique_entity_type_count']\n",
      "[Fold 3 end] Memory: 2055.1 MB (13.1%)\n",
      "\n",
      "LGB FOLD 4/5\n",
      "[Fold 4 start] Memory: 2055.1 MB (13.1%)\n",
      "Train: (11885709, 27), Val: (2971427, 27)\n",
      "Starting LightGBM training...\n",
      "[50]\tvalid_0's multi_logloss: 0.756676\n",
      "[100]\tvalid_0's multi_logloss: 0.732913\n",
      "[150]\tvalid_0's multi_logloss: 0.718369\n",
      "[200]\tvalid_0's multi_logloss: 0.708527\n",
      "[250]\tvalid_0's multi_logloss: 0.701409\n",
      "[300]\tvalid_0's multi_logloss: 0.695501\n",
      "[350]\tvalid_0's multi_logloss: 0.691001\n",
      "[400]\tvalid_0's multi_logloss: 0.68718\n",
      "[450]\tvalid_0's multi_logloss: 0.683962\n",
      "[500]\tvalid_0's multi_logloss: 0.68089\n",
      "[550]\tvalid_0's multi_logloss: 0.678355\n",
      "[600]\tvalid_0's multi_logloss: 0.675766\n",
      "[650]\tvalid_0's multi_logloss: 0.673499\n",
      "[700]\tvalid_0's multi_logloss: 0.671584\n",
      "[750]\tvalid_0's multi_logloss: 0.669894\n",
      "[800]\tvalid_0's multi_logloss: 0.668346\n",
      "[850]\tvalid_0's multi_logloss: 0.666791\n",
      "[900]\tvalid_0's multi_logloss: 0.665552\n",
      "[950]\tvalid_0's multi_logloss: 0.664368\n",
      "[1000]\tvalid_0's multi_logloss: 0.663234\n",
      "Training completed in: 0:20:45.326763\n",
      "Saved LightGBM fold 4\n",
      "Fold 4 - Macro F1: 0.61916, Weighted F1: 0.61842\n",
      "\n",
      "Top 10 features: ['OrgId', 'OrgId_freq', 'DetectorId', 'incident_duration_seconds', 'AlertTitle', 'evidence_rate', 'evidence_count', 'alert_rate', 'category_InitialAccess_count', 'unique_entity_type_count']\n",
      "[Fold 4 end] Memory: 721.1 MB (4.6%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LGB folds:  80%|████████  | 4/5 [1:22:37<20:51, 1251.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LGB FOLD 5/5\n",
      "[Fold 5 start] Memory: 919.1 MB (5.9%)\n",
      "Train: (11885709, 27), Val: (2971427, 27)\n",
      "Starting LightGBM training...\n",
      "[50]\tvalid_0's multi_logloss: 0.75586\n",
      "[100]\tvalid_0's multi_logloss: 0.732254\n",
      "[150]\tvalid_0's multi_logloss: 0.718298\n",
      "[200]\tvalid_0's multi_logloss: 0.708338\n",
      "[250]\tvalid_0's multi_logloss: 0.700654\n",
      "[300]\tvalid_0's multi_logloss: 0.694826\n",
      "[350]\tvalid_0's multi_logloss: 0.690144\n",
      "[400]\tvalid_0's multi_logloss: 0.686526\n",
      "[450]\tvalid_0's multi_logloss: 0.683024\n",
      "[500]\tvalid_0's multi_logloss: 0.679993\n",
      "[550]\tvalid_0's multi_logloss: 0.67736\n",
      "[600]\tvalid_0's multi_logloss: 0.674944\n",
      "[650]\tvalid_0's multi_logloss: 0.672742\n",
      "[700]\tvalid_0's multi_logloss: 0.67082\n",
      "[750]\tvalid_0's multi_logloss: 0.668983\n",
      "[800]\tvalid_0's multi_logloss: 0.667524\n",
      "[850]\tvalid_0's multi_logloss: 0.66619\n",
      "[900]\tvalid_0's multi_logloss: 0.664913\n",
      "[950]\tvalid_0's multi_logloss: 0.66372\n",
      "[1000]\tvalid_0's multi_logloss: 0.662426\n",
      "Training completed in: 0:19:12.547230\n",
      "Saved LightGBM fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LGB folds: 100%|██████████| 5/5 [1:43:30<00:00, 1242.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 - Macro F1: 0.61978, Weighted F1: 0.61898\n",
      "\n",
      "Top 10 features: ['OrgId', 'OrgId_freq', 'DetectorId', 'incident_duration_seconds', 'AlertTitle', 'evidence_rate', 'evidence_count', 'category_InitialAccess_count', 'alert_rate', 'unique_entity_type_count']\n",
      "[Fold 5 end] Memory: 769.3 MB (4.9%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_idx, val_idx) in enumerate(tqdm(cv.split(X_selected, y), total=N_SPLITS, desc='LGB folds')):\n",
    "    print(f\"\\nLGB FOLD {fold+1}/{N_SPLITS}\")\n",
    "    print_memory_usage(f\"Fold {fold+1} start\")\n",
    "    \n",
    "    # Get fold data\n",
    "    X_train = X_selected.iloc[train_idx]\n",
    "    X_val = X_selected.iloc[val_idx]\n",
    "    y_train = y[train_idx]\n",
    "    y_val = y[val_idx]\n",
    "    \n",
    "    print(f\"Train: {X_train.shape}, Val: {X_val.shape}\")\n",
    "    \n",
    "    lgbm = lgb.LGBMClassifier(**lgb_params)\n",
    "    \n",
    "    print(\"Starting LightGBM training...\")\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    callbacks = [\n",
    "        lgb.early_stopping(stopping_rounds=50, verbose=False),\n",
    "        lgb.log_evaluation(period=50)  \n",
    "    ]\n",
    "    \n",
    "    lgbm.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric='multi_logloss',\n",
    "        categorical_feature=CATEGORICAL_FEATURES,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    train_time = datetime.now() - start_time\n",
    "    print(f\"Training completed in: {train_time}\")\n",
    "    \n",
    "    # Predict OOF\n",
    "    val_proba = lgbm.predict_proba(X_val)\n",
    "    oof_lgb[val_idx] = val_proba\n",
    "    \n",
    "    # Feature importance\n",
    "    importance = lgbm.feature_importances_\n",
    "    feature_importances_lgb[f'fold_{fold+1}'] = importance\n",
    "    \n",
    "    # Save model\n",
    "    lgb_model_path = LGB_DIR / f'lgb_fold{fold+1}_{TIMESTAMP}.pkl'\n",
    "    save_pickle(lgbm, lgb_model_path)\n",
    "    print(f'Saved LightGBM fold {fold+1}')\n",
    "    \n",
    "    # Evaluate\n",
    "    fold_pred = np.argmax(val_proba, axis=1)\n",
    "    f1_macro = f1_score(y_val, fold_pred, average='macro')\n",
    "    f1_weighted = f1_score(y_val, fold_pred, average='weighted')\n",
    "    print(f'Fold {fold+1} - Macro F1: {f1_macro:.5f}, Weighted F1: {f1_weighted:.5f}')\n",
    "    \n",
    "    # Show top features\n",
    "    imp_series = pd.Series(importance, index=X_selected.columns).sort_values(ascending=False)\n",
    "    print(f'\\nTop 10 features: {list(imp_series.head(10).index)}')\n",
    "    \n",
    "    # Clean up\n",
    "    del lgbm, X_train, X_val, y_train, y_val, val_proba, importance\n",
    "    gc.collect()\n",
    "    print_memory_usage(f\"Fold {fold+1} end\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ddc4532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM Overall Results\n",
      "Overall Macro F1: 0.61963\n",
      "Overall Weighted F1: 0.61889\n"
     ]
    }
   ],
   "source": [
    "# Overall results\n",
    "oof_lgb_preds = np.argmax(oof_lgb, axis=1)\n",
    "overall_f1_macro_lgb = f1_score(y, oof_lgb_preds, average='macro')\n",
    "overall_f1_weighted_lgb = f1_score(y, oof_lgb_preds, average='weighted')\n",
    "\n",
    "print(\"LightGBM Overall Results\")\n",
    "print(f'Overall Macro F1: {overall_f1_macro_lgb:.5f}')\n",
    "print(f\"Overall Weighted F1: {overall_f1_weighted_lgb:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13a42bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 LightGBM features:\n",
      " 1. OrgId                          10126.00 (±41.29)\n",
      " 2. OrgId_freq                      6666.20 (±63.32)\n",
      " 3. DetectorId                      5746.00 (±46.89)\n",
      " 4. incident_duration_seconds       5443.00 (±112.79)\n",
      " 5. AlertTitle                      4872.80 (±70.26)\n",
      " 6. evidence_rate                   4801.80 (±70.75)\n",
      " 7. evidence_count                  4224.80 (±51.89)\n",
      " 8. alert_rate                      3571.20 (±49.60)\n",
      " 9. category_InitialAccess_count    3517.00 (±45.20)\n",
      "10. unique_entity_type_count        3286.20 (±40.52)\n",
      "11. entity_User_count               2980.00 (±67.44)\n",
      "12. entity_MailMessage_count        2879.40 (±58.48)\n",
      "13. unique_org_id_count             2846.20 (±29.34)\n",
      "14. entity_Ip_count                 2810.20 (±57.58)\n",
      "15. entity_Machine_count            2614.00 (±31.55)\n",
      "16. unique_alert_count              2396.20 (±26.41)\n",
      "17. category_SuspiciousActivity_count  2382.20 (±25.36)\n",
      "18. unique_detector_id_count        2378.20 (±19.95)\n",
      "19. entity_File_count               2375.80 (±43.39)\n",
      "20. category_CommandAndControl_count  2197.40 (±29.45)\n"
     ]
    }
   ],
   "source": [
    "feature_importances_lgb['mean'] = feature_importances_lgb.mean(axis=1)\n",
    "feature_importances_lgb['std'] = feature_importances_lgb.std(axis=1)\n",
    "top_features_lgb = feature_importances_lgb.sort_values('mean', ascending=False)\n",
    "\n",
    "print('Top 20 LightGBM features:')\n",
    "for i, (feat, row) in enumerate(top_features_lgb.head(20).iterrows(), 1):\n",
    "    print(f\"{i:2d}. {feat:<30} {row['mean']:8.2f} (±{row['std']:5.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb5e93ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save it\n",
    "np.save(LGB_DIR / f'oof_lgb_{TIMESTAMP}.npy', oof_lgb)\n",
    "save_pickle(feature_importances_lgb, LGB_DIR / f'feature_importances_lgb_{TIMESTAMP}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19034a1d",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cef7515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section 3: XGBOOST Training\n"
     ]
    }
   ],
   "source": [
    "print('Section 3: XGBOOST Training')\n",
    "\n",
    "cv = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "oof_xgb = np.zeros((len(X_selected), num_classes), dtype=np.float32)\n",
    "feature_importances_xgb = pd.DataFrame(index=X_selected.columns)\n",
    "\n",
    "xgb_params = {\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': num_classes,\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1,\n",
    "    'min_child_weight': 5,\n",
    "    'gamma': 0.1,\n",
    "    'tree_method': 'hist',    \n",
    "    'device': 'cuda',         \n",
    "    'max_bin': 256,\n",
    "    'eval_metric': 'mlogloss'\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a66ecf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "XGB folds:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB FOLD 1/5\n",
      "[XGB FOLD 1 start] Memory: 6286.3 MB (40.1%)\n",
      "Starting XGBoost training...\n",
      "[0]\ttrain-mlogloss:1.06491\teval-mlogloss:1.06494\n",
      "[50]\ttrain-mlogloss:0.78980\teval-mlogloss:0.79016\n",
      "[100]\ttrain-mlogloss:0.75778\teval-mlogloss:0.75828\n",
      "[150]\ttrain-mlogloss:0.73757\teval-mlogloss:0.73814\n",
      "[200]\ttrain-mlogloss:0.72360\teval-mlogloss:0.72422\n",
      "[250]\ttrain-mlogloss:0.71235\teval-mlogloss:0.71304\n",
      "[300]\ttrain-mlogloss:0.70388\teval-mlogloss:0.70463\n",
      "[350]\ttrain-mlogloss:0.69769\teval-mlogloss:0.69848\n",
      "[400]\ttrain-mlogloss:0.69220\teval-mlogloss:0.69304\n",
      "[450]\ttrain-mlogloss:0.68748\teval-mlogloss:0.68837\n",
      "[500]\ttrain-mlogloss:0.68350\teval-mlogloss:0.68443\n",
      "[550]\ttrain-mlogloss:0.67986\teval-mlogloss:0.68086\n",
      "[600]\ttrain-mlogloss:0.67650\teval-mlogloss:0.67757\n",
      "[650]\ttrain-mlogloss:0.67372\teval-mlogloss:0.67486\n",
      "[700]\ttrain-mlogloss:0.67103\teval-mlogloss:0.67223\n",
      "[750]\ttrain-mlogloss:0.66849\teval-mlogloss:0.66975\n",
      "[800]\ttrain-mlogloss:0.66609\teval-mlogloss:0.66741\n",
      "[850]\ttrain-mlogloss:0.66406\teval-mlogloss:0.66544\n",
      "[900]\ttrain-mlogloss:0.66222\teval-mlogloss:0.66365\n",
      "[950]\ttrain-mlogloss:0.66024\teval-mlogloss:0.66172\n",
      "[999]\ttrain-mlogloss:0.65852\teval-mlogloss:0.66005\n",
      "XGBoost training completed in: 0:02:37.592319\n",
      "Saved XGBoost fold 1\n",
      "Fold 1 - Macro F1: 0.60779, Weighted F1: 0.61784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "XGB folds:  20%|██        | 1/5 [03:07<12:29, 187.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[XGB Fold 1 end] Memory: 5157.9 MB (32.9%)\n",
      "XGB FOLD 2/5\n",
      "[XGB FOLD 2 start] Memory: 5398.7 MB (34.5%)\n",
      "Starting XGBoost training...\n",
      "[0]\ttrain-mlogloss:1.06543\teval-mlogloss:1.06543\n",
      "[50]\ttrain-mlogloss:0.79014\teval-mlogloss:0.78991\n",
      "[100]\ttrain-mlogloss:0.75587\teval-mlogloss:0.75569\n",
      "[150]\ttrain-mlogloss:0.73668\teval-mlogloss:0.73656\n",
      "[200]\ttrain-mlogloss:0.72255\teval-mlogloss:0.72250\n",
      "[250]\ttrain-mlogloss:0.71234\teval-mlogloss:0.71234\n",
      "[300]\ttrain-mlogloss:0.70375\teval-mlogloss:0.70380\n",
      "[350]\ttrain-mlogloss:0.69729\teval-mlogloss:0.69741\n",
      "[400]\ttrain-mlogloss:0.69178\teval-mlogloss:0.69198\n",
      "[450]\ttrain-mlogloss:0.68734\teval-mlogloss:0.68760\n",
      "[500]\ttrain-mlogloss:0.68318\teval-mlogloss:0.68351\n",
      "[550]\ttrain-mlogloss:0.67972\teval-mlogloss:0.68011\n",
      "[600]\ttrain-mlogloss:0.67634\teval-mlogloss:0.67680\n",
      "[650]\ttrain-mlogloss:0.67329\teval-mlogloss:0.67381\n",
      "[700]\ttrain-mlogloss:0.67079\teval-mlogloss:0.67138\n",
      "[750]\ttrain-mlogloss:0.66811\teval-mlogloss:0.66878\n",
      "[800]\ttrain-mlogloss:0.66582\teval-mlogloss:0.66654\n",
      "[850]\ttrain-mlogloss:0.66392\teval-mlogloss:0.66470\n",
      "[900]\ttrain-mlogloss:0.66187\teval-mlogloss:0.66273\n",
      "[950]\ttrain-mlogloss:0.66017\teval-mlogloss:0.66108\n",
      "[999]\ttrain-mlogloss:0.65852\teval-mlogloss:0.65950\n",
      "XGBoost training completed in: 0:02:37.190647\n",
      "Saved XGBoost fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "XGB folds:  40%|████      | 2/5 [06:14<09:21, 187.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 - Macro F1: 0.60746, Weighted F1: 0.61759\n",
      "[XGB Fold 2 end] Memory: 6320.0 MB (40.4%)\n",
      "XGB FOLD 3/5\n",
      "[XGB FOLD 3 start] Memory: 6480.0 MB (41.4%)\n",
      "Starting XGBoost training...\n",
      "[0]\ttrain-mlogloss:1.06492\teval-mlogloss:1.06494\n",
      "[50]\ttrain-mlogloss:0.78841\teval-mlogloss:0.78865\n",
      "[100]\ttrain-mlogloss:0.75606\teval-mlogloss:0.75629\n",
      "[150]\ttrain-mlogloss:0.73486\teval-mlogloss:0.73508\n",
      "[200]\ttrain-mlogloss:0.72230\teval-mlogloss:0.72255\n",
      "[250]\ttrain-mlogloss:0.71207\teval-mlogloss:0.71235\n",
      "[300]\ttrain-mlogloss:0.70355\teval-mlogloss:0.70390\n",
      "[350]\ttrain-mlogloss:0.69703\teval-mlogloss:0.69743\n",
      "[400]\ttrain-mlogloss:0.69203\teval-mlogloss:0.69248\n",
      "[450]\ttrain-mlogloss:0.68739\teval-mlogloss:0.68790\n",
      "[500]\ttrain-mlogloss:0.68331\teval-mlogloss:0.68387\n",
      "[550]\ttrain-mlogloss:0.67972\teval-mlogloss:0.68034\n",
      "[600]\ttrain-mlogloss:0.67659\teval-mlogloss:0.67727\n",
      "[650]\ttrain-mlogloss:0.67374\teval-mlogloss:0.67447\n",
      "[700]\ttrain-mlogloss:0.67093\teval-mlogloss:0.67172\n",
      "[750]\ttrain-mlogloss:0.66836\teval-mlogloss:0.66921\n",
      "[800]\ttrain-mlogloss:0.66594\teval-mlogloss:0.66685\n",
      "[850]\ttrain-mlogloss:0.66379\teval-mlogloss:0.66475\n",
      "[900]\ttrain-mlogloss:0.66187\teval-mlogloss:0.66290\n",
      "[950]\ttrain-mlogloss:0.65997\teval-mlogloss:0.66105\n",
      "[999]\ttrain-mlogloss:0.65826\teval-mlogloss:0.65939\n",
      "XGBoost training completed in: 0:02:37.924111\n",
      "Saved XGBoost fold 3\n",
      "Fold 3 - Macro F1: 0.60787, Weighted F1: 0.61805\n",
      "[XGB Fold 3 end] Memory: 6438.7 MB (41.1%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "XGB folds:  60%|██████    | 3/5 [09:19<06:12, 186.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB FOLD 4/5\n",
      "[XGB FOLD 4 start] Memory: 6438.7 MB (41.1%)\n",
      "Starting XGBoost training...\n",
      "[0]\ttrain-mlogloss:1.06549\teval-mlogloss:1.06554\n",
      "[50]\ttrain-mlogloss:0.78975\teval-mlogloss:0.78998\n",
      "[100]\ttrain-mlogloss:0.75824\teval-mlogloss:0.75855\n",
      "[150]\ttrain-mlogloss:0.73669\teval-mlogloss:0.73706\n",
      "[200]\ttrain-mlogloss:0.72268\teval-mlogloss:0.72311\n",
      "[250]\ttrain-mlogloss:0.71145\teval-mlogloss:0.71198\n",
      "[300]\ttrain-mlogloss:0.70319\teval-mlogloss:0.70377\n",
      "[350]\ttrain-mlogloss:0.69651\teval-mlogloss:0.69714\n",
      "[400]\ttrain-mlogloss:0.69135\teval-mlogloss:0.69201\n",
      "[450]\ttrain-mlogloss:0.68653\teval-mlogloss:0.68723\n",
      "[500]\ttrain-mlogloss:0.68266\teval-mlogloss:0.68341\n",
      "[550]\ttrain-mlogloss:0.67931\teval-mlogloss:0.68011\n",
      "[600]\ttrain-mlogloss:0.67623\teval-mlogloss:0.67708\n",
      "[650]\ttrain-mlogloss:0.67331\teval-mlogloss:0.67421\n",
      "[700]\ttrain-mlogloss:0.67078\teval-mlogloss:0.67173\n",
      "[750]\ttrain-mlogloss:0.66828\teval-mlogloss:0.66928\n",
      "[800]\ttrain-mlogloss:0.66601\teval-mlogloss:0.66707\n",
      "[850]\ttrain-mlogloss:0.66386\teval-mlogloss:0.66495\n",
      "[900]\ttrain-mlogloss:0.66199\teval-mlogloss:0.66315\n",
      "[950]\ttrain-mlogloss:0.66001\teval-mlogloss:0.66122\n",
      "[999]\ttrain-mlogloss:0.65832\teval-mlogloss:0.65959\n",
      "XGBoost training completed in: 0:02:39.520801\n",
      "Saved XGBoost fold 4\n",
      "Fold 4 - Macro F1: 0.60770, Weighted F1: 0.61761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "XGB folds:  80%|████████  | 4/5 [12:34<03:09, 189.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[XGB Fold 4 end] Memory: 4834.5 MB (30.9%)\n",
      "XGB FOLD 5/5\n",
      "[XGB FOLD 5 start] Memory: 5086.0 MB (32.5%)\n",
      "Starting XGBoost training...\n",
      "[0]\ttrain-mlogloss:1.06563\teval-mlogloss:1.06557\n",
      "[50]\ttrain-mlogloss:0.79048\teval-mlogloss:0.79007\n",
      "[100]\ttrain-mlogloss:0.75434\teval-mlogloss:0.75392\n",
      "[150]\ttrain-mlogloss:0.73490\teval-mlogloss:0.73454\n",
      "[200]\ttrain-mlogloss:0.72187\teval-mlogloss:0.72158\n",
      "[250]\ttrain-mlogloss:0.71144\teval-mlogloss:0.71125\n",
      "[300]\ttrain-mlogloss:0.70339\teval-mlogloss:0.70328\n",
      "[350]\ttrain-mlogloss:0.69695\teval-mlogloss:0.69691\n",
      "[400]\ttrain-mlogloss:0.69178\teval-mlogloss:0.69180\n",
      "[450]\ttrain-mlogloss:0.68715\teval-mlogloss:0.68724\n",
      "[500]\ttrain-mlogloss:0.68283\teval-mlogloss:0.68301\n",
      "[550]\ttrain-mlogloss:0.67942\teval-mlogloss:0.67965\n",
      "[600]\ttrain-mlogloss:0.67635\teval-mlogloss:0.67666\n",
      "[650]\ttrain-mlogloss:0.67339\teval-mlogloss:0.67377\n",
      "[700]\ttrain-mlogloss:0.67075\teval-mlogloss:0.67120\n",
      "[750]\ttrain-mlogloss:0.66830\teval-mlogloss:0.66881\n",
      "[800]\ttrain-mlogloss:0.66600\teval-mlogloss:0.66658\n",
      "[850]\ttrain-mlogloss:0.66366\teval-mlogloss:0.66430\n",
      "[900]\ttrain-mlogloss:0.66176\teval-mlogloss:0.66247\n",
      "[950]\ttrain-mlogloss:0.65998\teval-mlogloss:0.66075\n",
      "[999]\ttrain-mlogloss:0.65843\teval-mlogloss:0.65926\n",
      "XGBoost training completed in: 0:02:37.843287\n",
      "Saved XGBoost fold 5\n",
      "Fold 5 - Macro F1: 0.60785, Weighted F1: 0.61801\n",
      "[XGB Fold 5 end] Memory: 6503.4 MB (41.5%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "XGB folds: 100%|██████████| 5/5 [15:43<00:00, 188.74s/it]\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_idx, val_idx) in enumerate(tqdm(cv.split(X_selected, y), total=N_SPLITS, desc='XGB folds')):\n",
    "    print(f\"XGB FOLD {fold+1}/{N_SPLITS}\")\n",
    "    print_memory_usage(f\"XGB FOLD {fold+1} start\")\n",
    "    \n",
    "    X_train = X_selected.iloc[train_idx].copy()\n",
    "    X_val = X_selected.iloc[val_idx].copy()\n",
    "    y_train = y[train_idx]\n",
    "    y_val = y[val_idx]\n",
    "\n",
    "    # Label encoding categorical features\n",
    "    label_encoders = {}\n",
    "    for col in CATEGORICAL_FEATURES:\n",
    "        if col in X_train.columns:\n",
    "            le = LabelEncoder()\n",
    "            combined_data = pd.concat([X_train[col].astype(str), X_val[col].astype(str)])\n",
    "            le.fit(combined_data)\n",
    "            X_train[col] = le.transform(X_train[col].astype(str))\n",
    "            X_val[col] = le.transform(X_val[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "\n",
    "    for c in X_train.select_dtypes(include=[np.number]).columns:\n",
    "        X_train[c] = X_train[c].astype(np.float32)\n",
    "        X_val[c] = X_val[c].astype(np.float32)\n",
    "\n",
    "    # Convert to DMatrix\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train, feature_names=X_train.columns.tolist())\n",
    "    dval = xgb.DMatrix(X_val, label=y_val, feature_names=X_val.columns.tolist())\n",
    "    evals = [(dtrain, 'train'), (dval, 'eval')]\n",
    "    \n",
    "    print(\"Starting XGBoost training...\")\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    booster = xgb.train(\n",
    "        params=xgb_params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=1000,          \n",
    "        evals=evals,\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=50\n",
    "    )\n",
    "    \n",
    "    train_time = datetime.now() - start_time\n",
    "    print(f\"XGBoost training completed in: {train_time}\")\n",
    "\n",
    "    # Predict OOF\n",
    "    xgb_val_proba = booster.predict(dval)\n",
    "    oof_xgb[val_idx] = xgb_val_proba\n",
    "\n",
    "    # Feature importance\n",
    "    importance_dict = booster.get_score(importance_type='weight')\n",
    "    importance = pd.Series(importance_dict, index=X_train.columns).reindex(X_selected.columns).fillna(0)\n",
    "    feature_importances_xgb[f'fold_{fold+1}'] = importance\n",
    "    \n",
    "    # Save model + encoders\n",
    "    xgb_pkl_path = XGB_DIR / f'xgb_fold{fold+1}_{TIMESTAMP}.pkl'\n",
    "    save_pickle({'model': booster, 'label_encoders': label_encoders}, xgb_pkl_path)\n",
    "    print(f'Saved XGBoost fold {fold+1}')\n",
    "    \n",
    "    # Evaluate\n",
    "    fold_pred = np.argmax(xgb_val_proba, axis=1)\n",
    "    f1_macro = f1_score(y_val, fold_pred, average='macro')\n",
    "    f1_weighted = f1_score(y_val, fold_pred, average='weighted')\n",
    "    print(f'Fold {fold+1} - Macro F1: {f1_macro:.5f}, Weighted F1: {f1_weighted:.5f}')\n",
    "    \n",
    "    # Clean up\n",
    "    del booster, X_train, X_val, y_train, y_val, dtrain, dval, label_encoders\n",
    "    gc.collect()\n",
    "    print_memory_usage(f\"XGB Fold {fold+1} end\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5a82330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Overall Results\n",
      "Overall Macro F1: 0.60774\n",
      "Overall Weighted F1: 0.61782\n"
     ]
    }
   ],
   "source": [
    "oof_xgb_preds = np.argmax(oof_xgb, axis=1)\n",
    "overall_f1_macro_xgb = f1_score(y, oof_xgb_preds, average='macro')\n",
    "overall_f1_weighted_xgb = f1_score(y, oof_xgb_preds, average='weighted')\n",
    "\n",
    "print(\"XGBoost Overall Results\")\n",
    "print(f'Overall Macro F1: {overall_f1_macro_xgb:.5f}')\n",
    "print(f'Overall Weighted F1: {overall_f1_weighted_xgb:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bb04116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Feature Importances:\n",
      "OrgId_freq                           15591.2\n",
      "incident_duration_seconds             9289.0\n",
      "evidence_count                        8978.8\n",
      "evidence_rate                         7675.2\n",
      "OrgId                                 6293.0\n",
      "category_InitialAccess_count          6017.6\n",
      "unique_entity_type_count              5956.0\n",
      "alert_rate                            5886.2\n",
      "DetectorId_freq                       5558.0\n",
      "AlertTitle_freq                       5528.0\n",
      "entity_Ip_count                       5516.8\n",
      "entity_User_count                     5381.8\n",
      "unique_alert_count                    5030.4\n",
      "unique_org_id_count                   4993.2\n",
      "entity_MailMessage_count              4690.8\n",
      "unique_detector_id_count              4502.2\n",
      "entity_Machine_count                  4296.4\n",
      "entity_File_count                     4078.8\n",
      "category_SuspiciousActivity_count     3513.8\n",
      "unique_mitre_techniques_count         3061.6\n",
      "Name: mean_importance, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "feature_importances_xgb[\"mean_importance\"] = feature_importances_xgb.mean(axis=1)\n",
    "\n",
    "# Sort by importance\n",
    "top20_features = feature_importances_xgb[\"mean_importance\"].sort_values(ascending=False).head(20)\n",
    "\n",
    "print(\"Top 20 Feature Importances:\")\n",
    "print(top20_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b8af9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "np.save(XGB_DIR / f'oof_xgb_{TIMESTAMP}.npy', oof_xgb)\n",
    "save_pickle(feature_importances_xgb, XGB_DIR / f'feature_importances_xgb_{TIMESTAMP}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d82472",
   "metadata": {},
   "source": [
    "# Final Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d251fd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Ensemble\n",
    "PROCESSED_FEATURES_PATH = '../data/processed/incident_features.parquet'\n",
    "RAW_FEATURES_PATH = '../data/raw/GUIDE_Train.parquet'\n",
    "TARGET = 'IncidentGrade'\n",
    "OUT_DIR = Path('models')\n",
    "LGB_DIR = OUT_DIR / 'lightgbm'\n",
    "XGB_DIR = OUT_DIR / 'xgboost'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362b6104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recreating the target variable 'y' with the correct join logic...\n",
      "Target variable 'y' is ready. Shape: (14857136,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Recreating the target variable 'y' with the correct join logic...\")\n",
    "df_features = pl.read_parquet(PROCESSED_FEATURES_PATH)\n",
    "raw_df = pl.read_parquet(RAW_FEATURES_PATH).select([\n",
    "    'IncidentId', 'OrgId', 'DetectorId', 'AlertTitle', 'EntityType', 'Category'\n",
    "])\n",
    "\n",
    "df_for_y = df_features.join(raw_df, on='IncidentId', how='left')\n",
    "\n",
    "df_pandas = df_for_y.to_pandas()\n",
    "y, class_names = pd.factorize(df_pandas[TARGET])\n",
    "\n",
    "print(f\"Target variable 'y' is ready. Shape: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a693e42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading saved OOF prediction files...\n",
      "Found LightGBM OOF file: models\\lightgbm\\oof_lgb_20250925_123541.npy\n",
      "Found XGBoost OOF file: models\\xgboost\\oof_xgb_20250925_193826.npy\n",
      "Loaded OOF predictions. Shape: (14857136, 3)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "print(\"\\nLoading saved OOF prediction files...\")\n",
    "list_of_lgb_files = glob.glob(os.path.join(LGB_DIR, 'oof_lgb_*.npy'))\n",
    "latest_lgb_file = max(list_of_lgb_files, key=os.path.getctime)\n",
    "print(f\"Found LightGBM OOF file: {latest_lgb_file}\")\n",
    "\n",
    "list_of_xgb_files = glob.glob(os.path.join(XGB_DIR, 'oof_xgb_*.npy'))\n",
    "latest_xgb_file = max(list_of_xgb_files, key=os.path.getctime)\n",
    "print(f\"Found XGBoost OOF file: {latest_xgb_file}\")\n",
    "\n",
    "# Load the numpy arrays\n",
    "oof_lgb = np.load(latest_lgb_file)\n",
    "oof_xgb = np.load(latest_xgb_file)\n",
    "print(f\"Loaded OOF predictions. Shape: {oof_lgb.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f11507c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Ensemble Calculation\n",
      "\n",
      "Final Comparison (Macro F1):\n",
      "LightGBM: 0.61963\n",
      "XGBoost:  0.60774\n",
      "Ensemble: 0.62343\n"
     ]
    }
   ],
   "source": [
    "print('\\nFinal Ensemble Calculation')\n",
    "\n",
    "# Recalculate individual model scores for comparison\n",
    "overall_f1_macro_lgb = f1_score(y, np.argmax(oof_lgb, axis=1), average='macro')\n",
    "overall_f1_macro_xgb = f1_score(y, np.argmax(oof_xgb, axis=1), average='macro')\n",
    "\n",
    "# Create the ensemble by averaging the predictions\n",
    "oof_ensemble = (oof_lgb + oof_xgb) / 2.0\n",
    "oof_ens_preds = np.argmax(oof_ensemble, axis=1)\n",
    "f1_ens_macro = f1_score(y, oof_ens_preds, average='macro')\n",
    "\n",
    "print('\\nFinal Comparison (Macro F1):')\n",
    "print(f\"LightGBM: {overall_f1_macro_lgb:.5f}\")\n",
    "print(f\"XGBoost:  {overall_f1_macro_xgb:.5f}\")\n",
    "print(f\"Ensemble: {f1_ens_macro:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58e85fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RECOMMENDATION: Use LightGBM\n"
     ]
    }
   ],
   "source": [
    "improvement_threshold = 0.005\n",
    "best_single = max(overall_f1_macro_lgb, overall_f1_macro_xgb)\n",
    "if f1_ens_macro > best_single + improvement_threshold:\n",
    "    recommendation = \"ENSEMBLE\"\n",
    "elif overall_f1_macro_lgb > overall_f1_macro_xgb:\n",
    "    recommendation = \"LightGBM\"\n",
    "else:\n",
    "    recommendation = \"XGBoost\"\n",
    "\n",
    "print(f'RECOMMENDATION: Use {recommendation}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dbb8243d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Final] Memory: 2346.7 MB (15.0%)\n",
      "\n",
      "TRAINING COMPLETED SUCCESSFULLY!\n",
      "All models saved with timestamp: 20250925_193826\n"
     ]
    }
   ],
   "source": [
    "# Save final results\n",
    "results_summary = {\n",
    "    'lightgbm_macro_f1': float(overall_f1_macro_lgb),\n",
    "    'xgboost_macro_f1': float(overall_f1_macro_xgb), \n",
    "    'ensemble_macro_f1': float(f1_ens_macro),\n",
    "    'recommendation': recommendation,\n",
    "    'timestamp': TIMESTAMP\n",
    "}\n",
    "\n",
    "np.save(ENS_DIR / f'oof_ensemble_{TIMESTAMP}.npy', oof_ensemble)\n",
    "save_pickle(results_summary, ENS_DIR / f'results_summary_{TIMESTAMP}.pkl')\n",
    "\n",
    "print_memory_usage(\"Final\")\n",
    "print('\\nTRAINING COMPLETED SUCCESSFULLY!')\n",
    "print(f'All models saved with timestamp: {TIMESTAMP}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6c52e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
